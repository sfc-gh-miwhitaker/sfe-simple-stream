# Testing Guide - Simple Stream

**Time to Complete:** ~3 minutes (automated) or ~10 minutes (manual)
**Prerequisites:** Completed [`02-DEPLOYMENT.md`](02-DEPLOYMENT.md)

---

## Purpose

This guide shows you how to test your Simple Stream pipeline end-to-end using the provided event simulator.

## Choose Your Path

### Path 1: Automated Testing (Recommended) - 3 Minutes

**One command does everything:** RSA keys -> SQL execution -> Python setup -> Event sending

```bash
./tools/02_setup_and_test.sh
# Paste your Snowflake account identifier when prompted
```

**That's it.** Skip to [Step 3: Verify Data Flow](#step-3-verify-data-flow)

---

### Path 2: Manual Testing - 10 Minutes

**For learning or troubleshooting:** Follow all steps manually to understand each component.

Continue reading below.

---

## Testing Overview (Manual Path)

You'll complete three steps:
1. **Configure Authentication** - Create service account and RSA key pair
2. **Send Test Events** - Use the provided simulator to stream sample data
3. **Verify Data Flow** - Query tables to confirm end-to-end processing

**Total Time:** ~10 minutes

---

## What the Automated Script Does

**The `./tools/02_setup_and_test.sh` script performs these actions:**

1. **Generate RSA Key Pair** -> Creates `.secrets/keys/rsa_key.p8` (private) and `rsa_key.pub` (public)
2. **Generate SQL Script** -> Creates `.secrets/configure_auth_READY.sql` with embedded public key
3. **Display Instructions** -> Shows you exactly what to paste into Snowsight
4. **Setup Python Environment** -> Creates virtual environment, installs SDK
5. **Create Config File** -> Generates `.secrets/config.json` with your account identifier
6. **Send Test Events** -> Streams 10 sample events to verify end-to-end flow

**Total Time:** ~2 minutes of setup + ~1 minute of execution = ~3 minutes

**What You Do:**
1. Run the script
2. Paste your account identifier when prompted (from `deploy_all.sql` output)
3. Copy/paste the generated SQL into Snowsight (one time)
4. Watch events flow through the pipeline

---

## Step 1: Run Automated Test (Recommended Path)

**Unix/macOS:**
```bash
./tools/02_setup_and_test.sh
```

**Windows:**
```cmd
.\tools\02_setup_and_test.bat
```

**Follow the prompts:**

```
========================================
Simple Stream - Automated Setup & Test
========================================

Step 1: Account Identifier
--------------------------
Enter your Snowflake account identifier
(Format: orgname-accountname, from deploy_all.sql output)

Account: #
```

**Paste your account** (e.g., `myorg-myaccount`) and press Enter.

**The script will:**
- OK Generate RSA key pair -> `.secrets/keys/rsa_key.p8`, `rsa_key.pub`
- OK Create SQL script -> `.secrets/configure_auth_READY.sql` (with your public key embedded)
- OK Display instructions for Snowsight

**You'll see:**

```
========================================
Step 2: Configure Authentication in Snowflake
========================================

 COPY THIS ENTIRE BLOCK TO SNOWSIGHT:

   File: .secrets/configure_auth_READY.sql

   1. Open Snowsight
   2. Create new worksheet
   3. Paste the ENTIRE file contents
   4. Click "Run All"
   5. Verify "OK Authentication configured" message

Press Enter when SQL execution is complete...
```

**In Snowsight:**
1. Open the file `.secrets/configure_auth_READY.sql` (generated by script)
2. Copy entire contents
3. Paste into new Snowsight worksheet
4. Click **"Run All"**
5. Verify you see: `OK Authentication configured`

**Return to terminal and press Enter.**

**The script continues:**
- OK Creates Python virtual environment
- OK Installs `snowflake-ingest` SDK
- OK Generates `config.json` with your account
- OK Sends 10 test events

**Expected Output:**
```
========================================
Step 3: Python Environment Setup
========================================
OK Virtual environment created (.venv)
OK Activated virtual environment
OK Installing Snowflake Ingest SDK...
OK SDK installed successfully

========================================
Step 4: Configuration File
========================================
OK Created config.json with your account details
  Account: myorg-myaccount
  User: SFE_INGEST_USER
  Target: SNOWFLAKE_EXAMPLE.RAW_INGESTION.RAW_BADGE_EVENTS

========================================
Step 5: Send Test Events
========================================
OK Loading configuration...
OK Initializing Snowpipe Streaming SDK...
OK SDK authenticated (JWT handled automatically)
OK Streaming 10 events...
OK Successfully sent 10 events

OK SUCCESS - All events delivered

Verify in Snowsight:
  SELECT COUNT(*) FROM SNOWFLAKE_EXAMPLE.RAW_INGESTION.RAW_BADGE_EVENTS;
```

**-> Skip to [Step 3: Verify Data Flow](#step-3-verify-data-flow)**

---

## Step 2: Manual Testing (Alternative Path)

**Use this path if:**
- The automated script fails
- You want to understand each component
- You're troubleshooting authentication issues
- You're learning the SDK workflow

### 2.1 Generate RSA Key Pair

The Snowpipe Streaming API uses key-pair authentication for enhanced security.

**Unix/macOS:**
```bash
cd .secrets
mkdir -p keys
openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out keys/rsa_key.p8 -nocrypt
openssl rsa -in keys/rsa_key.p8 -pubout -outform PEM > keys/rsa_key.pub
```

**Windows (PowerShell or Git Bash):**
```powershell
cd .secrets
mkdir keys
openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out keys/rsa_key.p8 -nocrypt
openssl rsa -in keys/rsa_key.p8 -pubout -outform PEM > keys/rsa_key.pub
```

**WARNING Security Note:**
- Keep `rsa_key.p8` secure (never commit, never email)
- Set permissions: `chmod 600 keys/rsa_key.p8` (Unix/Mac)

### 2.2 Configure Authentication in Snowflake

Create SQL script manually:

**File: `.secrets/configure_auth_MANUAL.sql`**

```sql
USE ROLE SECURITYADMIN;

-- Create service account
CREATE USER IF NOT EXISTS SFE_INGEST_USER
  COMMENT = 'DEMO: Snowpipe Streaming SDK user | Expires: 2026-02-05';

-- Register public key (paste contents of keys/rsa_key.pub)
ALTER USER SFE_INGEST_USER
  SET RSA_PUBLIC_KEY = '-----BEGIN PUBLIC KEY-----
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA...
-----END PUBLIC KEY-----';

-- Grant role
GRANT ROLE sfe_ingest_role TO USER SFE_INGEST_USER;

-- Grant privileges
USE ROLE SYSADMIN;
GRANT USAGE ON DATABASE SNOWFLAKE_EXAMPLE TO ROLE sfe_ingest_role;
GRANT USAGE ON SCHEMA SNOWFLAKE_EXAMPLE.RAW_INGESTION TO ROLE sfe_ingest_role;
GRANT INSERT ON TABLE SNOWFLAKE_EXAMPLE.RAW_INGESTION.RAW_BADGE_EVENTS TO ROLE sfe_ingest_role;
GRANT OPERATE ON PIPE SNOWFLAKE_EXAMPLE.RAW_INGESTION.SFE_BADGE_EVENTS_PIPE TO ROLE sfe_ingest_role;

-- Verify
SHOW GRANTS TO ROLE sfe_ingest_role;
```

**Run in Snowsight** -> Should see `OK Grants confirmed`

### 2.3 Setup Python Environment Manually

```bash
cd .secrets
python3 -m venv .venv
source .venv/bin/activate  # Unix/macOS
# .venv\Scripts\activate   # Windows
pip install -r ../simulator/requirements.txt
```

### 2.4 Create Config File Manually

**File: `.secrets/config.json`**

```json
{
  "account": "myorg-myaccount",
  "user": "SFE_INGEST_USER",
  "role": "sfe_ingest_role",
  "private_key_path": "keys/rsa_key.p8",
  "database": "SNOWFLAKE_EXAMPLE",
  "schema": "RAW_INGESTION",
  "table": "RAW_BADGE_EVENTS",
  "sample_events": 10
}
```

Replace `myorg-myaccount` with your actual account identifier.

### 2.5 Run Simulator Manually

```bash
cd simulator
./send_events.sh      # Unix/macOS
# send_events.bat     # Windows
```

**Expected:** `OK SUCCESS: All events delivered to Snowflake`

---

## Step 3: Verify Data Flow

### Check Raw Table

Verify events landed in the raw table:

```sql
USE SCHEMA RAW_INGESTION;

SELECT
    badge_id,
    user_id,
    zone_id,
    event_timestamp,
    signal_strength,
    signal_quality,
    direction,
    ingestion_time
FROM RAW_BADGE_EVENTS
ORDER BY ingestion_time DESC
LIMIT 10;
```

**Expected Output:**
- 10 rows (or however many you sent)
- Recent `ingestion_time` (within last few seconds)
- Valid `badge_id`, `user_id`, `zone_id` values
- `signal_quality` computed from `signal_strength`

### Activate Tasks

Tasks are deployed but suspended. Activate them to process data:

```sql
-- Resume tasks (must be in correct order: parent first, then child)
ALTER TASK sfe_raw_to_staging_task RESUME;
ALTER TASK sfe_staging_to_analytics_task RESUME;
```

**Verify tasks are running:**
```sql
SHOW TASKS IN SCHEMA RAW_INGESTION;
```

Look for `STATE = started` in the output.

### Wait for Task Execution

Tasks run every 1 minute. Wait up to 2 minutes for:
- Task 1: Dedupe raw -> staging
- Task 2: Enrich staging -> analytics

**Check task execution history:**
```sql
SELECT
    name,
    state,
    scheduled_time,
    completed_time,
    DATEDIFF('second', scheduled_time, completed_time) AS duration_sec
FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(
    TASK_NAME => 'sfe_raw_to_staging_task',
    SCHEDULED_TIME_RANGE_START => DATEADD('hour', -1, CURRENT_TIMESTAMP())
))
ORDER BY scheduled_time DESC
LIMIT 5;
```

**Expected:** Recent successful execution (state = 'SUCCEEDED')

### Check Staging Table

Verify deduplication worked:

```sql
SELECT
    badge_id,
    user_id,
    zone_id,
    event_timestamp,
    processed_time
FROM STAGING_LAYER.STG_BADGE_EVENTS
ORDER BY processed_time DESC
LIMIT 10;
```

**Expected Output:**
- 10 rows (duplicates removed if any)
- `processed_time` populated by task
- No duplicate (badge_id, event_timestamp) pairs

### Check Analytics Fact Table

Verify dimensional enrichment worked:

```sql
SELECT
    a.event_sk,
    a.user_id,
    u.user_name,
    u.department,
    a.zone_id,
    z.zone_name,
    z.zone_type,
    a.event_timestamp,
    a.direction,
    a.signal_strength,
    a.dwell_time_minutes
FROM ANALYTICS_LAYER.FCT_ACCESS_EVENTS a
LEFT JOIN ANALYTICS_LAYER.DIM_USERS u
    ON a.user_id = u.user_id AND u.is_current = TRUE
LEFT JOIN ANALYTICS_LAYER.DIM_ZONES z
    ON a.zone_id = z.zone_id AND z.is_current = TRUE
ORDER BY a.event_timestamp DESC
LIMIT 10;
```

**Expected Output:**
- 10 enriched rows
- `user_name` and `department` populated from DIM_USERS
- `zone_name` and `zone_type` populated from DIM_ZONES
- `dwell_time_minutes` calculated if applicable

---

## Monitoring Queries

### End-to-End Latency

Check how long data takes to flow through the pipeline:

```sql
SELECT
  layer,
  last_update,
  seconds_since_update,
  row_count,
  health_status
FROM RAW_INGESTION.V_END_TO_END_LATENCY
ORDER BY seconds_since_update DESC
LIMIT 10;
```

**Expected Latency:**
- Raw -> Staging: ~60 seconds (1-minute task schedule)
- Staging -> Analytics: ~60 seconds (1-minute task schedule)
- **Total End-to-End:** < 2 minutes

### Ingestion Metrics

View ingestion rate and volume:

```sql
SELECT
  ingestion_hour,
  event_count,
  events_per_second,
  unique_badges,
  unique_zones,
  avg_signal_strength,
  weak_signal_count,
  weak_signal_pct,
  entry_count,
  exit_count,
  net_occupancy_change
FROM RAW_INGESTION.V_INGESTION_METRICS
ORDER BY ingestion_hour DESC
LIMIT 24;
```

**Shows:**
- Events per hour
- Average events per minute
- Peak ingestion times

### Task Execution History

Monitor task success/failure rates:

```sql
SELECT
  task_name,
  state,
  scheduled_time,
  completed_time,
  duration_seconds,
  error_code,
  error_message,
  execution_status
FROM RAW_INGESTION.V_TASK_EXECUTION_HISTORY
ORDER BY scheduled_time DESC
LIMIT 10;
```

**Look for:**
- State: SUCCEEDED (green)
- Duration: < 5 seconds (for demo volume)
- Error_message: NULL (no errors)

### Data Quality Metrics

Check for duplicates, orphans, and data quality issues:

```sql
SELECT
  total_raw_events,
  total_staged_events,
  total_fact_events,
  duplicate_count,
  duplicate_rate_pct,
  orphan_user_count,
  orphan_zone_count,
  orphan_rate_pct,
  weak_signal_count,
  weak_signal_rate_pct
FROM RAW_INGESTION.V_DATA_QUALITY_METRICS;
```

**Expected:**
- Duplicate_rate: 0-5% (normal for RFID systems)
- Orphan_rate: 0% (all user_id and zone_id should match dimensions)
- Weak_signal_rate: < 10%

---

## Troubleshooting

### Authentication Issues

#### "SDK Authentication Failed"

**Symptom:** Error during SDK initialization or streaming
**Cause:** RSA public key not registered or doesn't match private key

**Debug Steps:**

1. **Verify public key is registered:**
```sql
DESCRIBE USER SFE_INGEST_USER;
-- Look for RSA_PUBLIC_KEY_FP field (should be set)
```

2. **Verify user has correct role:**
```sql
SHOW GRANTS TO USER sfe_ingest_user;
-- Should show sfe_ingest_role granted
```

3. **Verify role has correct privileges:**
```sql
SHOW GRANTS TO ROLE sfe_ingest_role;
-- Should show INSERT privilege on table
```

**Fix Authentication:**
```sql
-- Re-register the public key
-- Unix/macOS: cat keys/rsa_key.pub.b64
-- Windows: type keys\rsa_key.pub.b64

ALTER USER SFE_INGEST_USER
  SET RSA_PUBLIC_KEY = '<paste single-line base64 string>';
```

**Why SDK Simplifies This:**
- SDK calculates fingerprint automatically (no manual calculation)
- SDK generates JWT tokens internally (no JWT encoding errors)
- SDK validates credentials on connection (immediate feedback)

### Simulator Issues

#### "Config file not found"

**Symptom:** `FileNotFoundError: config.json`
**Cause:** Config file not created or wrong directory
**Fix:**
```bash
cd .secrets
ls config.json  # Should exist
```

If missing, run the setup script:
```bash
./tools/02_setup_and_test.sh --step 4
```

#### "Module 'snowflake' not found"

**Symptom:** `ModuleNotFoundError: No module named 'snowflake'`
**Cause:** Virtual environment not activated or SDK not installed
**Fix:**
```bash
# Activate venv
source .secrets/.venv/bin/activate  # Unix/macOS
.secrets\.venv\Scripts\activate     # Windows

# Install SDK
pip install -r simulator/requirements.txt
```

#### "Connection timeout"

**Symptom:** Request times out after 30 seconds
**Cause:** Network firewall blocking HTTPS:443 to Snowflake
**Fix:** Verify outbound HTTPS is allowed to `*.snowflakecomputing.com`

### Data Flow Issues

#### "No data in RAW_BADGE_EVENTS"

**Symptom:** Table empty after running simulator
**Cause:** Pipe error or insufficient privileges
**Check:**
```sql
-- Check pipe status
SHOW PIPES IN SCHEMA RAW_INGESTION;
-- Look for EXECUTION_STATE

-- Check pipe history
SELECT
  file_name,
  last_load_time,
  status,
  row_count,
  error_count,
  first_error_message
FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(
    TABLE_NAME => 'RAW_INGESTION.RAW_BADGE_EVENTS',
    START_TIME => DATEADD('hour', -1, CURRENT_TIMESTAMP())
));

-- Check for pipe errors
SHOW PIPE ERRORS IN PIPE sfe_badge_events_pipe;
```

#### "Tasks not executing"

**Symptom:** No rows in staging or analytics tables
**Cause:** Tasks still suspended or erroring
**Check:**
```sql
-- Verify tasks are resumed
SHOW TASKS IN SCHEMA RAW_INGESTION;
-- STATE should be 'started', not 'suspended'

-- Check task errors
SELECT
  name,
  state,
  scheduled_time,
  completed_time,
  error_code,
  error_message,
  query_id
FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY())
WHERE NAME IN ('sfe_raw_to_staging_task', 'sfe_staging_to_analytics_task')
  AND STATE = 'FAILED'
ORDER BY SCHEDULED_TIME DESC
LIMIT 10;
```

**Fix:** Resume tasks:
```sql
ALTER TASK sfe_raw_to_staging_task RESUME;
ALTER TASK sfe_staging_to_analytics_task RESUME;
```

#### "Orphan records in fact table"

**Symptom:** FCT_ACCESS_EVENTS has NULL user_name or zone_name
**Cause:** user_id or zone_id in events doesn't match dimension tables
**Check:**
```sql
-- Find orphaned events
SELECT
    badge_id,
    user_id,
    zone_id
FROM RAW_BADGE_EVENTS
WHERE user_id NOT IN (SELECT user_id FROM ANALYTICS_LAYER.DIM_USERS WHERE is_current = TRUE)
   OR zone_id NOT IN (SELECT zone_id FROM ANALYTICS_LAYER.DIM_ZONES WHERE is_current = TRUE);
```

**Fix:** Use sample IDs that exist in dimension tables:
- Users: USR-001, USR-002, USR-003
- Zones: ZONE-LOBBY-1, ZONE-OFFICE-201, ZONE-LAB-301, ZONE-EXIT-1

---

## Testing Checklist

- [ ] RSA key pair generated
- [ ] Public key registered with Snowflake
- [ ] Fingerprint matches between simulator and Snowflake
- [ ] Python virtual environment created and activated
- [ ] Required packages installed
- [ ] `config.json` created with correct account details
- [ ] Simulator runs without errors
- [ ] Events appear in RAW_BADGE_EVENTS
- [ ] Tasks resumed and executing successfully
- [ ] Events flow to STG_BADGE_EVENTS (deduplicated)
- [ ] Events flow to FCT_ACCESS_EVENTS (enriched)
- [ ] End-to-end latency < 2 minutes
- [ ] No orphan records (all FKs resolve)
- [ ] Monitoring views show metrics

**Estimated Total Testing Time:** 10 minutes

---

## Stress Testing (Optional)

To test higher volumes, increase `sample_events` in `config.json`:

```json
{
  ...
  "sample_events": 1000
}
```

**Volume Guidelines:**
- 10 events: Basic smoke test (default)
- 100 events: Stress test tasks and transformations
- 1,000 events: Performance testing
- 10,000+ events: Scale testing (may take several minutes)

**Monitor Performance:**
```sql
-- Check warehouse credit consumption
SELECT
  warehouse_name,
  start_time,
  end_time,
  credits_used
FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY
WHERE WAREHOUSE_NAME = 'COMPUTE_WH'
  AND START_TIME >= DATEADD('hour', -1, CURRENT_TIMESTAMP())
ORDER BY START_TIME DESC;

-- Check task duration trends
SELECT
    NAME,
    AVG(DATEDIFF('second', SCHEDULED_TIME, COMPLETED_TIME)) AS avg_duration_sec,
    MAX(DATEDIFF('second', SCHEDULED_TIME, COMPLETED_TIME)) AS max_duration_sec,
    COUNT(*) AS execution_count
FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(
    SCHEDULED_TIME_RANGE_START => DATEADD('hour', -1, CURRENT_TIMESTAMP())
))
WHERE NAME IN ('sfe_raw_to_staging_task', 'sfe_staging_to_analytics_task')
GROUP BY NAME;
```

---

## What's Next?

OK **Testing Complete!**

**-> Next:** [`04-MONITORING.md`](04-MONITORING.md) - Learn how to monitor the pipeline in production

**For Data Providers:**
- [`06-DATA-PROVIDER-QUICKSTART.md`](06-DATA-PROVIDER-QUICKSTART.md) - Send this 1-page guide to vendors
- [`05-API-HANDOFF.md`](05-API-HANDOFF.md) - Complete API reference for troubleshooting

---

## Related Documentation

### Internal Team
- [`01-SETUP.md`](01-SETUP.md) - Prerequisites and setup
- [`02-DEPLOYMENT.md`](02-DEPLOYMENT.md) - Pipeline deployment
- [`04-MONITORING.md`](04-MONITORING.md) - Next: Monitor pipeline health

### External Data Providers
- [`06-DATA-PROVIDER-QUICKSTART.md`](06-DATA-PROVIDER-QUICKSTART.md) - **Send to vendors** (10-minute integration)
- [`05-API-HANDOFF.md`](05-API-HANDOFF.md) - Complete API reference (troubleshooting)
